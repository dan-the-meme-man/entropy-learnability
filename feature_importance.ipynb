{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(json_file):\n",
    "    j = json.load(open(json_file))\n",
    "    settings = os.path.basename(json_file).replace('.json', '').split('_')\n",
    "    j['dist'] = settings[0]\n",
    "    j['uni_or_bi'] = settings[1]\n",
    "    j['vocab_size'] = int(settings[2])\n",
    "    j['softmax'] = True if settings[3] == 'softmax' else False\n",
    "    j['settings'] = settings\n",
    "    j['losses'] = []\n",
    "    if 'lstm' in settings:\n",
    "        j['model_type'] = 'lstm'\n",
    "    elif 'ffnn' in settings:\n",
    "        j['model_type'] = 'ffnn'\n",
    "    else:\n",
    "        j['model_type'] = 'trf'\n",
    "    if 'embd256' in settings:\n",
    "        j['embd_size'] = 256\n",
    "    else:\n",
    "        j['embd_size'] = 64\n",
    "    for i in range(len(j['train_losses'])):\n",
    "        j['losses'].extend(j['train_losses'][i])\n",
    "    del j['train_losses']\n",
    "    if 'val_losses' in j:\n",
    "        print(json_file)\n",
    "    return j\n",
    "\n",
    "json_files = [\n",
    "    os.path.join('results', x) for x in os.listdir('results') if x.endswith('.json')\n",
    "]\n",
    "\n",
    "jsons = [data(json_file) for json_file in json_files]\n",
    "\n",
    "test_set_perplexity = []\n",
    "entropy = []\n",
    "dist = []\n",
    "uni_or_bi = []\n",
    "vocab_size = []\n",
    "softmax = []\n",
    "model_type = []\n",
    "embd_size = []\n",
    "\n",
    "for j in jsons:\n",
    "    test_set_perplexity.append(min(j['test_set_perplexities']))\n",
    "    entropy.append(j['entropy'])\n",
    "    dist.append(j['dist'])\n",
    "    uni_or_bi.append(j['uni_or_bi'])\n",
    "    vocab_size.append(j['vocab_size'])\n",
    "    softmax.append(j['softmax'])\n",
    "    model_type.append(j['model_type'])\n",
    "    embd_size.append(j['embd_size'])\n",
    "    \n",
    "for i in range(len(entropy)):\n",
    "    if uni_or_bi[i] == 'bigrams':\n",
    "        entropy[i] = entropy[i] / 2\n",
    "    \n",
    "test_set_avg_cross_entropy = [math.log(x) for x in test_set_perplexity]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Test set perplexity': test_set_perplexity,\n",
    "    'Test set average cross-entropy': test_set_avg_cross_entropy,\n",
    "    'Entropy': entropy,\n",
    "    'Distribution': dist,\n",
    "    'Uni- or bigram': uni_or_bi,\n",
    "    'Vocab size': vocab_size,\n",
    "    'Softmax': softmax,\n",
    "    'Model type': model_type,\n",
    "    'Embedding size': embd_size\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OrdinalEncoder()\n",
    "df['Distribution'] = enc.fit_transform(df[['Distribution']])\n",
    "df['Uni- or bigram'] = enc.fit_transform(df[['Uni- or bigram']])\n",
    "df['Softmax'] = enc.fit_transform(df[['Softmax']])\n",
    "df['Model type'] = enc.fit_transform(df[['Model type']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  OLS Regression Results                                  \n",
      "==========================================================================================\n",
      "Dep. Variable:     Test set average cross-entropy   R-squared:                       0.999\n",
      "Model:                                        OLS   Adj. R-squared:                  0.999\n",
      "Method:                             Least Squares   F-statistic:                 3.657e+04\n",
      "Date:                            Tue, 10 Dec 2024   Prob (F-statistic):               0.00\n",
      "Time:                                    21:36:42   Log-Likelihood:                 394.74\n",
      "No. Observations:                             357   AIC:                            -773.5\n",
      "Df Residuals:                                 349   BIC:                            -742.5\n",
      "Df Model:                                       7                                         \n",
      "Covariance Type:                        nonrobust                                         \n",
      "==================================================================================\n",
      "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "const             -0.1259      0.020     -6.411      0.000      -0.165      -0.087\n",
      "Entropy            1.0228      0.003    306.817      0.000       1.016       1.029\n",
      "Distribution       0.0014      0.006      0.226      0.821      -0.011       0.014\n",
      "Uni- or bigram    -0.0118      0.009     -1.308      0.192      -0.030       0.006\n",
      "Vocab size     -3.146e-06   2.36e-06     -1.334      0.183   -7.78e-06    1.49e-06\n",
      "Softmax            0.0344      0.010      3.362      0.001       0.014       0.055\n",
      "Model type         0.0021      0.006      0.340      0.734      -0.010       0.014\n",
      "Embedding size -2.824e-06   4.75e-05     -0.059      0.953   -9.62e-05    9.06e-05\n",
      "==============================================================================\n",
      "Omnibus:                       14.920   Durbin-Watson:                   0.535\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               35.258\n",
      "Skew:                          -0.045   Prob(JB):                     2.21e-08\n",
      "Kurtosis:                       4.537   Cond. No.                     1.69e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.69e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "reg = sm.OLS(\n",
    "    df['Test set average cross-entropy'],\n",
    "    sm.add_constant(df.drop(columns=['Test set average cross-entropy', 'Test set perplexity']))\n",
    ")\n",
    "res = reg.fit()\n",
    "print(res.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
