{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_attrs(list1, list2):\n",
    "    return [str(list1[i]) + ', ' + str(list2[i]) for i in range(len(list1))]\n",
    "\n",
    "def data(json_file):\n",
    "    j = json.load(open(json_file))\n",
    "    del j['train_losses']\n",
    "    del j['table']\n",
    "    settings = os.path.basename(json_file).replace('.json', '').split('_')\n",
    "    if settings[0] == 'long':\n",
    "        del settings[0]\n",
    "        settings[0] = 'long_range'\n",
    "    j['dist'] = settings[0]\n",
    "    j['uni_or_bi'] = settings[1]\n",
    "    j['vocab_size'] = int(settings[2])\n",
    "    j['softmax'] = True if settings[3] == 'softmax' else False\n",
    "    j['settings'] = settings\n",
    "    if 'lstm' in settings:\n",
    "        j['model_type'] = 'lstm'\n",
    "    elif 'ffnn' in settings:\n",
    "        j['model_type'] = 'ffnn'\n",
    "    else:\n",
    "        j['model_type'] = 'trf'\n",
    "    if '256' in settings and 'embd' in settings:\n",
    "        j['embd_size'] = 256\n",
    "    else:\n",
    "        j['embd_size'] = 64\n",
    "    if 'val_losses' in j:\n",
    "        print(json_file)\n",
    "    return j\n",
    "\n",
    "json_files = paths = glob.glob('results/u*.json') + glob.glob('results/m*.json') + glob.glob('results/n*.json') + glob.glob('results/long*.json')\n",
    "\n",
    "jsons = []\n",
    "for json_file in json_files:\n",
    "    try:\n",
    "        d = data(json_file)\n",
    "        jsons.append(d)\n",
    "    except:\n",
    "        print(json_file)\n",
    "\n",
    "test_set_perplexity = []\n",
    "entropy = []\n",
    "dist = []\n",
    "uni_or_bi = []\n",
    "vocab_size = []\n",
    "softmax = []\n",
    "model_type = []\n",
    "embd_size = []\n",
    "\n",
    "for j in jsons:\n",
    "    test_set_perplexity.append(min(j['test_set_perplexities']))\n",
    "    entropy.append(j['entropy'])\n",
    "    dist.append(j['dist'])\n",
    "    uni_or_bi.append(j['uni_or_bi'])\n",
    "    vocab_size.append(j['vocab_size'])\n",
    "    softmax.append(j['softmax'])\n",
    "    model_type.append(j['model_type'])\n",
    "    embd_size.append(j['embd_size'])\n",
    "    \n",
    "for i in range(len(entropy)):\n",
    "    if uni_or_bi[i] == 'bigrams':\n",
    "        entropy[i] = entropy[i] / 2\n",
    "    \n",
    "test_set_avg_cross_entropy = [math.log(x) for x in test_set_perplexity]\n",
    "\n",
    "df = {\n",
    "    'Test set perplexity': test_set_perplexity,\n",
    "    'Test set average cross-entropy': test_set_avg_cross_entropy,\n",
    "    'Entropy': entropy,\n",
    "    'Distribution': dist,\n",
    "    'Uni- or bigram': uni_or_bi,\n",
    "    'Vocab size': vocab_size,\n",
    "    'Softmax': softmax,\n",
    "    'Model type': model_type,\n",
    "    'Embedding size': embd_size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {\n",
    "    'Test set perplexity': test_set_perplexity,\n",
    "    'Test set average cross-entropy': test_set_avg_cross_entropy,\n",
    "    'Entropy': entropy,\n",
    "    'Distribution': dist,\n",
    "    'Uni- or bigram': uni_or_bi,\n",
    "    'Vocab size': vocab_size,\n",
    "    'Softmax': softmax,\n",
    "    'Model type': model_type,\n",
    "    'Embedding size': embd_size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('results/data_for_modeling.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OrdinalEncoder()\n",
    "df['Distribution'] = enc.fit_transform(df[['Distribution']])\n",
    "df['Uni- or bigram'] = enc.fit_transform(df[['Uni- or bigram']])\n",
    "df['Softmax'] = enc.fit_transform(df[['Softmax']])\n",
    "df['Model type'] = enc.fit_transform(df[['Model type']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zscale numerical columns\n",
    "df = df.apply(lambda x: (x - x.mean()) / x.std() if x.name not in ['Distribution', 'Uni- or bigram', 'Softmax', 'Model type'] else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  OLS Regression Results                                  \n",
      "==========================================================================================\n",
      "Dep. Variable:     Test set average cross-entropy   R-squared:                       0.990\n",
      "Model:                                        OLS   Adj. R-squared:                  0.990\n",
      "Method:                             Least Squares   F-statistic:                     5950.\n",
      "Date:                            Tue, 04 Mar 2025   Prob (F-statistic):               0.00\n",
      "Time:                                    14:18:47   Log-Likelihood:                 374.40\n",
      "No. Observations:                             416   AIC:                            -732.8\n",
      "Df Residuals:                                 408   BIC:                            -700.6\n",
      "Df Model:                                       7                                         \n",
      "Covariance Type:                        nonrobust                                         \n",
      "==================================================================================\n",
      "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "const             -0.0779      0.012     -6.680      0.000      -0.101      -0.055\n",
      "Entropy            1.0285      0.008    137.074      0.000       1.014       1.043\n",
      "Distribution       0.0515      0.005     10.808      0.000       0.042       0.061\n",
      "Uni- or bigram    -0.0790      0.011     -6.920      0.000      -0.101      -0.057\n",
      "Vocab size        -0.0601      0.007     -8.053      0.000      -0.075      -0.045\n",
      "Softmax            0.0563      0.011      5.353      0.000       0.036       0.077\n",
      "Model type        -0.0062      0.010     -0.639      0.523      -0.025       0.013\n",
      "Embedding size     0.0002      0.005      0.040      0.968      -0.009       0.010\n",
      "==============================================================================\n",
      "Omnibus:                      124.684   Durbin-Watson:                   1.103\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              511.100\n",
      "Skew:                          -1.268   Prob(JB):                    1.04e-111\n",
      "Kurtosis:                       7.801   Cond. No.                         7.33\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "reg = sm.OLS(\n",
    "    df['Test set average cross-entropy'],\n",
    "    sm.add_constant(df.drop(columns=['Test set average cross-entropy', 'Test set perplexity']))\n",
    ")\n",
    "res = reg.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const             7.865134578826737610492472323491e-11\n",
      "Entropy           0.000000000000000000000000000000e+00\n",
      "Distribution      4.102618449911355731440236761405e-24\n",
      "Uni- or bigram    1.752839494882939304539502605677e-11\n",
      "Vocab size        8.952711003910896366715297742457e-15\n",
      "Softmax           1.448775993863391750885935560511e-07\n",
      "Model type        5.233806389292354754871894328971e-01\n",
      "Embedding size    9.678307428845354110080734244548e-01\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "p_values = res.pvalues\n",
    "print(p_values.apply('{:.30e}'.format))  # Print with scientific notation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('results/data_for_paper.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
