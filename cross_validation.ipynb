{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from nltk.corpus import brown\n",
    "words = brown.sents()\n",
    "raw_text = '\\n'.join([' '.join(sent) for sent in words])\n",
    "raw_text = re.sub(r' +', ' ', raw_text)\n",
    "raw_text = re.sub(r' ([.,!?;:])', r'\\1', raw_text)\n",
    "raw_text = re.sub(r'`` ', r'\"', raw_text)\n",
    "raw_text = re.sub(r\" ''\", r'\"', raw_text)\n",
    "raw_text = re.sub(r'``', r'\"', raw_text)\n",
    "raw_text = re.sub(r\"''\", r'\"', raw_text)\n",
    "raw_text = re.sub(r'\\.\\.\\n', r'.\\n', raw_text)\n",
    "raw_text = re.sub(r'\\?\\?', '?', raw_text)\n",
    "raw_text = re.sub('!!', '!', raw_text)\n",
    "raw_text = re.sub('\\' ', '\\'', raw_text)\n",
    "raw_text = re.sub(' \\'', '\\'', raw_text)\n",
    "raw_text = re.sub(r'\\( ', '(', raw_text)\n",
    "raw_text = re.sub(r' \\)', ')', raw_text)\n",
    "raw_text = re.sub(r'\\[ ', '[', raw_text)\n",
    "raw_text = re.sub(r' \\]', ']', raw_text)\n",
    "raw_text = re.sub(r';;', ';', raw_text)\n",
    "raw_text = re.sub(r'::', ':', raw_text)\n",
    "raw_text = re.sub(r'\\,\\,', ',', raw_text)\n",
    "raw_text = re.sub(r'\\,\\,', ',', raw_text)\n",
    "raw_text = re.sub(r'\\'\\'', '\\' \\'', raw_text)\n",
    "raw_text = re.sub(r'\"\"', '\"', raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('brown_data', exist_ok=True)\n",
    "\n",
    "lines = raw_text.split('\\n')\n",
    "\n",
    "N_FOLDS = 10\n",
    "\n",
    "for i in range(N_FOLDS):\n",
    "    with open(f'brown_data/brown_{i}.txt', 'w+', encoding='utf-8') as f:\n",
    "        for j, line in enumerate(lines):\n",
    "            if j % N_FOLDS == i:\n",
    "                f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 35378,     4,  8999,    38,     2,     1,     1,     1,     1],\n",
       "        [    0, 11249,   621,   398,   759,     8,   147, 23902,    32,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    BertTokenizer,\n",
    "    RobertaTokenizer,\n",
    "    XLMRobertaTokenizer,\n",
    ")\n",
    "\n",
    "settings = {\n",
    "    'padding': 'longest',\n",
    "    'max_length': 128,\n",
    "    'truncation': True,\n",
    "    'return_tensors': 'pt',\n",
    "    'return_token_type_ids': False,\n",
    "}\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "xlmroberta_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "gpt2_tokenizer(['Hello, world!', 'How are you my dear friends?'], **settings)\n",
    "bert_tokenizer(['Hello, world!', 'How are you my dear friends?'], **settings)\n",
    "roberta_tokenizer(['Hello, world!', 'How are you my dear friends?'], **settings)\n",
    "xlmroberta_tokenizer(['Hello, world!', 'How are you my dear friends?'], **settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from train_model import train_and_test\n",
    "from model import get_model, get_optimizer, get_scheduler\n",
    "\n",
    "seed = 42\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "hparams = {\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'n_positions': settings['max_length'],\n",
    "    'n_embd': 64,\n",
    "    'n_layer': 4,\n",
    "    'n_head': 4,\n",
    "    'resid_pdrop': 0.05,\n",
    "    'embd_pdrop': 0.05,\n",
    "    'attn_pdrop': 0.05,\n",
    "    'summary_first_dropout': 0.05,\n",
    "    'bos_token_id': 0,\n",
    "    'eos_token_id': 1,\n",
    "    'batch_size': 4,\n",
    "    'sequence_length': 64,\n",
    "    'epochs': 4,\n",
    "    'learning_rate': 0.001,\n",
    "    'warmup_steps': 100,\n",
    "    'weight_decay': 0.01,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'log_interval': 100\n",
    "}\n",
    "\n",
    "for tokenizer, save_name in zip(\n",
    "    [gpt2_tokenizer, bert_tokenizer, roberta_tokenizer, xlmroberta_tokenizer],\n",
    "    ['gpt2', 'bert', 'roberta', 'xlmroberta']\n",
    "):\n",
    "    \n",
    "    hparams['vocab_size'] = tokenizer.vocab_size\n",
    "    \n",
    "    for split in range(N_FOLDS):\n",
    "        \n",
    "        test_data = []\n",
    "        with open(f'brown_data/brown_{split}.txt', 'r', encoding='utf-8') as f:\n",
    "            test_data = f.readlines()\n",
    "        \n",
    "        train_data = []\n",
    "        for i in range(N_FOLDS):\n",
    "            if i == split:\n",
    "                continue\n",
    "            train_data.extend(open(f'brown_data/brown_{i}.txt', 'r', encoding='utf-8').readlines())\n",
    "\n",
    "        def collate_fn(data):\n",
    "            return tokenizer(data, **settings)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_data,\n",
    "            batch_size=hparams['batch_size'],\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            test_data,\n",
    "            batch_size=hparams['batch_size'],\n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        optimizer = get_optimizer(model, hparams)\n",
    "        \n",
    "        model = get_model(**hparams)\n",
    "        \n",
    "        train_and_test(\n",
    "            model,\n",
    "            train_loader,\n",
    "            test_loader,\n",
    "            optimizer=optimizer,\n",
    "            epochs=hparams['epochs'],\n",
    "            log_interval=hparams['log_interval'],\n",
    "            save_name=save_name + f'_{split}',\n",
    "            scheduler=get_scheduler(optimizer, hparams['warmup_steps']),\n",
    "            device=hparams['device'],\n",
    "            entropy=0\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
