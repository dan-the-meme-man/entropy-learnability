{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from generate_sequences import generate_bigram_sequences_using_table\n",
    "from tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_bigram_seqs_to_convergence(\n",
    "    bigram_table: torch.Tensor,\n",
    "    max_length: int = 128,\n",
    "    batch_size: int = 256\n",
    "):\n",
    "    device = bigram_table.device  # Ensure everything stays on the same device\n",
    "    counts = torch.zeros(len(bigram_table), device=device, dtype=torch.float32)\n",
    "    n = bigram_table.shape[0]\n",
    "    p = torch.full((n,), 1 / n, device=device, dtype=torch.float32)\n",
    "\n",
    "    for i in range(1000 * n):\n",
    "        counts_sum = counts.sum().clamp(min=1e-8)  # Avoid division by zero\n",
    "        p_next = counts / counts_sum\n",
    "\n",
    "        if i % 50 == 0:  # Compute norm less frequently for speedup\n",
    "            norm = torch.norm(p_next - p)\n",
    "            print(f\"Iteration {i}, Norm: {norm.item():.6f}\")\n",
    "            if norm < 1e-8:\n",
    "                break\n",
    "        p = p_next\n",
    "\n",
    "        # Generate bigram sequences in larger batches to better utilize GPU\n",
    "        seqs = generate_bigram_sequences_using_table(batch_size, max_length, bigram_table)\n",
    "\n",
    "        # Ensure seqs is on the same device before calling `.unique`\n",
    "        seqs = seqs.to(device)\n",
    "        uc, uc_counts = seqs.unique(return_counts=True)\n",
    "\n",
    "        # GPU-optimized scatter_add_\n",
    "        counts.scatter_add_(0, uc, uc_counts.to(counts.dtype))\n",
    "\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_table = create_normal_bigram_table(10000).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Norm: 0.010000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msample_bigram_seqs_to_convergence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbigram_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 23\u001b[0m, in \u001b[0;36msample_bigram_seqs_to_convergence\u001b[1;34m(bigram_table, max_length, batch_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m p \u001b[38;5;241m=\u001b[39m p_next\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Generate bigram sequences in larger batches to better utilize GPU\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m seqs \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_bigram_sequences_using_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbigram_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Ensure seqs is on the same device before calling `.unique`\u001b[39;00m\n\u001b[0;32m     26\u001b[0m seqs \u001b[38;5;241m=\u001b[39m seqs\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\danie\\OneDrive\\Desktop\\Current\\Entropy Learnability\\generate_sequences.py:58\u001b[0m, in \u001b[0;36mgenerate_bigram_sequences_using_table\u001b[1;34m(batch_size, sequence_length, bigram_probs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sequence_length):\n\u001b[1;32m---> 58\u001b[0m         sequences[i, j] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbigram_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m sequences\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sequences\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample_bigram_seqs_to_convergence(bigram_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "0.00711576733738184\n",
      "0.005210084840655327\n",
      "0.00290660560131073\n",
      "0.0020947977900505066\n",
      "0.0020090974867343903\n",
      "0.0012562324991449714\n",
      "0.001393466955050826\n",
      "0.0012117131846025586\n",
      "0.001045257318764925\n",
      "0.0008443673723377287\n",
      "0.0010063119698315859\n",
      "0.0006478465511463583\n",
      "0.0005971979117020965\n",
      "0.0010532723972573876\n",
      "0.0007574147311970592\n",
      "0.00043739587999880314\n",
      "0.0005724529619328678\n",
      "0.0008251378312706947\n",
      "0.0005973120569251478\n",
      "0.0004623265704140067\n",
      "0.0005956720560789108\n",
      "0.0004878173931501806\n",
      "0.00043426259071566164\n",
      "0.0005230871611274779\n",
      "0.0004728665226139128\n",
      "0.00047515128972008824\n",
      "0.00050107337301597\n",
      "0.00032228868803940713\n",
      "0.00045693537686020136\n",
      "0.0005689171375706792\n",
      "0.0004072487645316869\n",
      "0.00027504703029990196\n",
      "0.0002496831875760108\n",
      "0.0002855987404473126\n",
      "0.00023584833252243698\n",
      "0.00035541149554774165\n",
      "0.00018394945072941482\n",
      "0.00034586648689582944\n",
      "0.00016097835032269359\n",
      "0.0003276806965004653\n",
      "0.00017654357361607254\n",
      "0.00022352708037942648\n",
      "0.0002697829040698707\n",
      "0.0002921372069977224\n",
      "0.00015106191858649254\n",
      "0.0002124952879967168\n",
      "0.00026345974765717983\n",
      "0.00026060748496092856\n",
      "0.000142568169394508\n",
      "0.00023916270583868027\n",
      "0.00021645237575285137\n",
      "0.0001318388822255656\n",
      "0.00019177282229065895\n",
      "0.00030286889523267746\n",
      "0.00021774560445919633\n",
      "0.0001753691176418215\n",
      "0.00014170260692480952\n",
      "0.00017254630802199244\n",
      "0.0001538568176329136\n",
      "0.00020176306134089828\n",
      "0.00014695907884743065\n",
      "0.00013104011304676533\n",
      "6.377768295351416e-05\n",
      "0.0001777655998012051\n",
      "0.00017509724420960993\n",
      "0.0002092667855322361\n",
      "0.00017903298430610448\n",
      "0.0001371655089315027\n",
      "0.00014978389663156122\n",
      "0.0001780477905413136\n",
      "0.00010324038157705218\n",
      "0.00019525214156601578\n",
      "0.00019612984033301473\n",
      "7.541010563727468e-05\n",
      "7.030439883237705e-05\n",
      "0.00012920537847094238\n",
      "6.515251152450219e-05\n",
      "0.00014654197730123997\n",
      "0.0001532173773739487\n",
      "0.0001157437072834\n",
      "0.00011657115101115778\n",
      "0.00014602503506466746\n",
      "9.846855391515419e-05\n",
      "0.0001392107515130192\n",
      "0.00010197173105552793\n",
      "0.00010363974433857948\n",
      "0.00011340439959894866\n",
      "0.00017804001981858164\n",
      "0.000173062871908769\n",
      "8.23530281195417e-05\n",
      "0.00015972186520230025\n",
      "0.00010137748176930472\n",
      "0.00014314739382825792\n",
      "7.954714965308085e-05\n",
      "8.879997039912269e-05\n",
      "0.00010742990707512945\n",
      "0.0001038157643051818\n",
      "0.00014517315139528364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0842, 0.0990, 0.1115, 0.1037, 0.0893, 0.1059, 0.0983, 0.1054, 0.1096,\n",
       "        0.0931])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_bigram_seqs_to_convergence(bigram_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor([0.0842, 0.0990, 0.1115, 0.1037, 0.0893, 0.1059, 0.0983, 0.1054, 0.1096,\n",
    "        0.0931])\n",
    "tensor([0.0839, 0.0991, 0.1112, 0.1040, 0.0891, 0.1056, 0.0985, 0.1060, 0.1097,\n",
    "        0.0929])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stationary_distribution(bigram_table: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get the stationary distribution of a bigram table.\n",
    "    \n",
    "    Args:\n",
    "        bigram_table: `torch.Tensor` - the bigram table.\n",
    "        \n",
    "    Returns:\n",
    "        `torch.Tensor` - the stationary distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    n = bigram_table.shape[0]\n",
    "    p = torch.ones(n) / n\n",
    "    for _ in range(10 * n):\n",
    "        p_next = p @ bigram_table\n",
    "        if torch.norm(p_next - p) < 1e-8:\n",
    "            break\n",
    "        p = p_next\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy_bigram(bigram_table: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Calculate entropy of a bigram table.\n",
    "    \n",
    "    Args:\n",
    "        bigram_table: `torch.Tensor` - the bigram table.\n",
    "        \n",
    "    Returns:\n",
    "        `float` - the entropy of the bigram table.\n",
    "    \"\"\"\n",
    "    \n",
    "    p = get_stationary_distribution(bigram_table)\n",
    "    \n",
    "    joint_probs = p[:, None] * bigram_table\n",
    "    \n",
    "    try:\n",
    "        t = -1 * (joint_probs * joint_probs.log()).sum()\n",
    "    except:\n",
    "        t = -1 * (joint_probs * (joint_probs + 1e-10).log()).sum()\n",
    "        \n",
    "    return t.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.556020736694336"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_entropy_bigram(bigram_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
