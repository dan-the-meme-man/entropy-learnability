{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entropy import calculate_entropy_bigram, calculate_entropy_unigram\n",
    "from tables import create_normal_bigram_table, create_normal_unigram_table, create_uniform_unigram_table\n",
    "from generate_sequences import *\n",
    "import torch\n",
    "from entropy_opt import get_dist, MSEAgainstEntropyLoss, MSEAgainstEntropyAndVarEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ent: 1, var: 1\n",
      "e-e_t: -0.0007412, v-v_t: -0.0009586\n",
      "\n",
      "ent: 1, var: 2\n",
      "e-e_t: -0.0008433, v-v_t: -0.0006278\n",
      "\n",
      "ent: 1, var: 4\n",
      "e-e_t: 0.0006463, v-v_t: -0.001249\n",
      "\n",
      "ent: 1, var: 8\n",
      "e-e_t: -0.0008797, v-v_t: -8.687e-06\n",
      "\n",
      "ent: 1, var: 16\n",
      "e-e_t: -0.9053, v-v_t: 0.1677\n",
      "\n",
      "ent: 1, var: 32\n",
      "e-e_t: -3.167, v-v_t: 9.832\n",
      "\n",
      "ent: 1, var: 64\n",
      "e-e_t: -3.283, v-v_t: 41.81\n",
      "\n",
      "ent: 1, var: 128\n",
      "e-e_t: -3.305, v-v_t: 105.8\n",
      "\n",
      "ent: 2, var: 1\n",
      "e-e_t: -0.0009563, v-v_t: -0.0007765\n",
      "\n",
      "ent: 2, var: 2\n",
      "e-e_t: -0.0008611, v-v_t: -0.0008127\n",
      "\n",
      "ent: 2, var: 4\n",
      "e-e_t: -0.0006504, v-v_t: -0.0009168\n",
      "\n",
      "ent: 2, var: 8\n",
      "e-e_t: -0.0006016, v-v_t: -0.0006217\n",
      "\n",
      "ent: 2, var: 16\n",
      "e-e_t: -0.001111, v-v_t: 0.0002098\n",
      "\n",
      "ent: 2, var: 32\n",
      "e-e_t: -2.213, v-v_t: 9.82\n",
      "\n",
      "ent: 2, var: 64\n",
      "e-e_t: -2.294, v-v_t: 41.81\n",
      "\n",
      "ent: 2, var: 128\n",
      "e-e_t: -2.31, v-v_t: 105.8\n",
      "\n",
      "ent: 4, var: 1\n",
      "e-e_t: -0.0005537, v-v_t: -0.000693\n",
      "\n",
      "ent: 4, var: 2\n",
      "e-e_t: -0.0007355, v-v_t: -0.0005702\n",
      "\n",
      "ent: 4, var: 4\n",
      "e-e_t: -0.001156, v-v_t: -0.0004652\n",
      "\n",
      "ent: 4, var: 8\n",
      "e-e_t: -0.001107, v-v_t: -0.000279\n",
      "\n",
      "ent: 4, var: 16\n",
      "e-e_t: -0.0008797, v-v_t: 2.869e-05\n",
      "\n",
      "ent: 4, var: 32\n",
      "e-e_t: -0.3054, v-v_t: 9.808\n",
      "\n",
      "ent: 4, var: 64\n",
      "e-e_t: -0.3167, v-v_t: 41.81\n",
      "\n",
      "ent: 4, var: 128\n",
      "e-e_t: -0.3189, v-v_t: 105.8\n",
      "\n",
      "ent: 8, var: 1\n",
      "e-e_t: -0.0008448, v-v_t: -0.0003274\n",
      "\n",
      "ent: 8, var: 2\n",
      "e-e_t: -0.0008554, v-v_t: -0.0001227\n",
      "\n",
      "ent: 8, var: 4\n",
      "e-e_t: 0.0008485, v-v_t: 0.0008446\n",
      "\n",
      "ent: 8, var: 8\n",
      "e-e_t: 0.0004903, v-v_t: 0.0001003\n",
      "\n",
      "ent: 8, var: 16\n",
      "e-e_t: 1.152, v-v_t: 0.2318\n",
      "\n",
      "ent: 8, var: 32\n",
      "e-e_t: 3.508, v-v_t: 9.838\n",
      "\n",
      "ent: 8, var: 64\n",
      "e-e_t: 3.638, v-v_t: 41.81\n",
      "\n",
      "ent: 8, var: 128\n",
      "e-e_t: 3.663, v-v_t: 105.8\n",
      "\n",
      "ent: 16, var: 1\n",
      "e-e_t: 6.79, v-v_t: 1.0\n",
      "\n",
      "ent: 16, var: 2\n",
      "e-e_t: 6.946, v-v_t: 0.9337\n",
      "\n",
      "ent: 16, var: 4\n",
      "e-e_t: 7.208, v-v_t: 0.9368\n",
      "\n",
      "ent: 16, var: 8\n",
      "e-e_t: 7.723, v-v_t: 1.063\n",
      "\n",
      "ent: 16, var: 16\n",
      "e-e_t: 8.883, v-v_t: 1.63\n",
      "\n",
      "ent: 16, var: 32\n",
      "e-e_t: 11.15, v-v_t: 10.1\n",
      "\n",
      "ent: 16, var: 64\n",
      "e-e_t: 11.55, v-v_t: 41.83\n",
      "\n",
      "ent: 16, var: 128\n",
      "e-e_t: 11.63, v-v_t: 105.8\n",
      "\n",
      "ent: 32, var: 1\n",
      "e-e_t: 22.79, v-v_t: 1.0\n",
      "\n",
      "ent: 32, var: 2\n",
      "e-e_t: 22.79, v-v_t: 2.0\n",
      "\n",
      "ent: 32, var: 4\n",
      "e-e_t: 22.79, v-v_t: 4.0\n",
      "\n",
      "ent: 32, var: 8\n",
      "e-e_t: 23.45, v-v_t: 3.101\n",
      "\n",
      "ent: 32, var: 16\n",
      "e-e_t: 24.48, v-v_t: 3.984\n",
      "\n",
      "ent: 32, var: 32\n",
      "e-e_t: 26.52, v-v_t: 11.2\n",
      "\n",
      "ent: 32, var: 64\n",
      "e-e_t: 27.37, v-v_t: 41.91\n",
      "\n",
      "ent: 32, var: 128\n",
      "e-e_t: 27.55, v-v_t: 105.8\n",
      "\n",
      "ent: 64, var: 1\n",
      "e-e_t: 54.79, v-v_t: 1.0\n",
      "\n",
      "ent: 64, var: 2\n",
      "e-e_t: 54.79, v-v_t: 2.0\n",
      "\n",
      "ent: 64, var: 4\n",
      "e-e_t: 54.79, v-v_t: 4.0\n",
      "\n",
      "ent: 64, var: 8\n",
      "e-e_t: 54.79, v-v_t: 8.0\n",
      "\n",
      "ent: 64, var: 16\n",
      "e-e_t: 55.88, v-v_t: 7.92\n",
      "\n",
      "ent: 64, var: 32\n",
      "e-e_t: 57.63, v-v_t: 14.08\n",
      "\n",
      "ent: 64, var: 64\n",
      "e-e_t: 59.0, v-v_t: 42.28\n",
      "\n",
      "ent: 64, var: 128\n",
      "e-e_t: 59.41, v-v_t: 105.9\n",
      "\n",
      "ent: 128, var: 1\n",
      "e-e_t: 118.8, v-v_t: 1.0\n",
      "\n",
      "ent: 128, var: 2\n",
      "e-e_t: 118.8, v-v_t: 2.0\n",
      "\n",
      "ent: 128, var: 4\n",
      "e-e_t: 118.8, v-v_t: 4.0\n",
      "\n",
      "ent: 128, var: 8\n",
      "e-e_t: 118.8, v-v_t: 8.0\n",
      "\n",
      "ent: 128, var: 16\n",
      "e-e_t: 118.8, v-v_t: 16.0\n",
      "\n",
      "ent: 128, var: 32\n",
      "e-e_t: 120.5, v-v_t: 19.79\n",
      "\n",
      "ent: 128, var: 64\n",
      "e-e_t: 122.3, v-v_t: 43.76\n",
      "\n",
      "ent: 128, var: 128\n",
      "e-e_t: 123.1, v-v_t: 106.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ent    = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "varent = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "\n",
    "for e in ent:\n",
    "    for v in varent:\n",
    "        p, e_t, v_t = get_dist(\n",
    "            MSEAgainstEntropyAndVarEntropyLoss(),\n",
    "            10000, # decent vocab size - a lot of freedom regarding E vs VE\n",
    "            e,\n",
    "            v,\n",
    "            do_logging = False\n",
    "        )\n",
    "        print(f'ent: {e}, var: {v}')\n",
    "        print(f'e-e_t: {e-e_t:.4}, v-v_t: {v-v_t:.4}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'vocab_size': 100,\n",
    "    'n_positions': 64,\n",
    "    'n_embd': 64, # 64, 256\n",
    "    'n_layer': 4,\n",
    "    'n_head': 4,\n",
    "    'resid_pdrop': 0.05,\n",
    "    'embd_pdrop': 0.05,\n",
    "    'attn_pdrop': 0.05,\n",
    "    'summary_first_dropout': 0.05,\n",
    "    'bos_token_id': 0,\n",
    "    'eos_token_id': 1,\n",
    "    'pad_token_id': 2,\n",
    "    'batch_size': 4,\n",
    "    'sequence_length': 64,\n",
    "    'epochs': 4,\n",
    "    'learning_rate': 0.001,\n",
    "    'warmup_steps': 100,\n",
    "    'weight_decay': 0.01,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'dist': 'normal_unigram',\n",
    "    'num_train_samples': 8000,\n",
    "    'num_test_samples': 2000,\n",
    "    'log_interval': 10,\n",
    "    'manual_option': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = generate_unigram_sequences_using_table(\n",
    "    hparams['batch_size'],\n",
    "    hparams['sequence_length'],\n",
    "    create_uniform_unigram_table(hparams['vocab_size']),\n",
    "    hparams['bos_token_id'],\n",
    "    hparams['eos_token_id'],\n",
    "    hparams['pad_token_id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e(x):\n",
    "    print(calculate_entropy_unigram(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = create_uniform_unigram_table(100)\n",
    "e(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 8192\n",
    "y = torch.tensor(\n",
    "    [0.8] + [0.2 / n] * n\n",
    ")\n",
    "print(y.sum())\n",
    "e(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 282\n",
    "y = torch.tensor(\n",
    "    [0.7] + [0.3 / n] * n\n",
    ")\n",
    "print(y.sum())\n",
    "e(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 59\n",
    "y = torch.tensor(\n",
    "    [0.6] + [0.4 / n] * n\n",
    ")\n",
    "print(y.sum())\n",
    "e(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25\n",
    "y = torch.tensor(\n",
    "    [0.5] + [0.5 / n] * n\n",
    ")\n",
    "print(y.sum())\n",
    "e(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
