{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-tuned probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entropy import calculate_entropy_bigram, calculate_entropy_unigram\n",
    "from tables import create_normal_bigram_table, create_normal_unigram_table, create_uniform_unigram_table\n",
    "from generate_sequences import *\n",
    "from model import get_ffnn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'vocab_size': 100,\n",
    "    'n_positions': 64,\n",
    "    'n_embd': 64, # 64, 256\n",
    "    'n_layer': 4,\n",
    "    'n_head': 4,\n",
    "    'resid_pdrop': 0.05,\n",
    "    'embd_pdrop': 0.05,\n",
    "    'attn_pdrop': 0.05,\n",
    "    'summary_first_dropout': 0.05,\n",
    "    'bos_token_id': 0,\n",
    "    'eos_token_id': 1,\n",
    "    'pad_token_id': 2,\n",
    "    'batch_size': 4,\n",
    "    'sequence_length': 64,\n",
    "    'epochs': 4,\n",
    "    'learning_rate': 0.001,\n",
    "    'warmup_steps': 100,\n",
    "    'weight_decay': 0.01,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'dist': 'normal_unigram',\n",
    "    'num_train_samples': 8000,\n",
    "    'num_test_samples': 2000,\n",
    "    'log_interval': 10,\n",
    "    'manual_option': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_ffnn(**hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = generate_unigram_sequences_using_table(\n",
    "    hparams['batch_size'],\n",
    "    hparams['sequence_length'],\n",
    "    create_uniform_unigram_table(hparams['vocab_size']),\n",
    "    hparams['bos_token_id'],\n",
    "    hparams['eos_token_id'],\n",
    "    hparams['pad_token_id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(seqs, labels=seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e(x):\n",
    "    print(calculate_entropy_unigram(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = create_uniform_unigram_table(10)\n",
    "e(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 8192\n",
    "y = torch.tensor(\n",
    "    [0.8] + [0.2 / n] * n\n",
    ")\n",
    "print(y.sum())\n",
    "e(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 282\n",
    "y = torch.tensor(\n",
    "    [0.7] + [0.3 / n] * n\n",
    ")\n",
    "print(y.sum())\n",
    "e(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 59\n",
    "y = torch.tensor(\n",
    "    [0.6] + [0.4 / n] * n\n",
    ")\n",
    "print(y.sum())\n",
    "e(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25\n",
    "y = torch.tensor(\n",
    "    [0.5] + [0.5 / n] * n\n",
    ")\n",
    "print(y.sum())\n",
    "e(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "y = torch.tensor(\n",
    "    [0.4] + [0.6 / n] * n\n",
    ")\n",
    "print(y.sum())\n",
    "e(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 11\n",
    "y = torch.tensor(\n",
    "    [0.3] + [0.7 / n] * n\n",
    ")\n",
    "print(y.sum())\n",
    "e(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_sequences import *\n",
    "from tables import create_normal_bigram_table, create_normal_unigram_table, create_uniform_unigram_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = create_normal_unigram_table(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_unigram_sequences_using_table(4, 8, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob.glob('results/u*.json') + glob.glob('results/m*.json') + glob.glob('results/n*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = sorted(list(set(paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            dist = os.path.splitext(os.path.basename(path))[0]\n",
    "            print(dist)\n",
    "            print(data['entropy'] - data['transient_entropy'], min(data['test_set_perplexities']))\n",
    "            print()\n",
    "        except:\n",
    "            print('no data:', path)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy Optimization Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entropy_opt import get_dist, MSEAgainstEntropyAndVarEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda import is_available\n",
    "device = 'cuda' if is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_var_crit = MSEAgainstEntropyAndVarEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "vocab size: 10\n",
      "sum of probabilities (should be 1): 0.9999999999999999\n",
      "-----------------------------------------------------\n",
      "desired entropy:    1.5\n",
      "true entropy:       1.4998426312544317\n",
      "-----------------------------------------------------\n",
      "desired varentropy: 1.5\n",
      "true varentropy:    1.4993950914847778\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "param_grid = {\n",
    "    'vocab_size': [10, 100, 1000, 10000],\n",
    "    'desired_entropy': [1.5, 3.0, 4.5, 6.0, 7.5, 9.0],\n",
    "    'desired_varent': [1.5, 3.0, 4.5, 6.0, 7.5, 9.0]\n",
    "}\n",
    "\n",
    "for vocab_size, desired_entropy, desired_varent in product(*param_grid.values()):\n",
    "    final_dist = get_dist(\n",
    "        ent_var_crit,\n",
    "        vocab_size,\n",
    "        desired_entropy,\n",
    "        desired_varent,\n",
    "        False,\n",
    "        1e-6\n",
    "    )\n",
    "    \n",
    "    print('-----------------------------------------------------')\n",
    "    print(f'vocab size: {vocab_size}')\n",
    "    print(f'sum of probabilities (should be 1): {final_dist.sum()}')\n",
    "    X = -final_dist.log()\n",
    "    E_X = (final_dist * X).sum()\n",
    "    E_X_sq = (final_dist * X * X).sum()\n",
    "    mean = E_X.item()\n",
    "    var = E_X_sq.item() - (mean ** 2)\n",
    "    print('-----------------------------------------------------')\n",
    "    print(f'desired entropy:    {desired_entropy}')\n",
    "    print(f'true entropy:       {mean}')\n",
    "    print('-----------------------------------------------------')\n",
    "    print(f'desired varentropy: {desired_varent}')\n",
    "    print(f'true varentropy:    {var}')\n",
    "    print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
